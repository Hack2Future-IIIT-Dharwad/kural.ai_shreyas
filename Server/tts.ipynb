{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from groq) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\91805\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->groq) (1.1.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Using cached groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq\n",
      "Successfully installed distro-1.9.0 groq-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91805\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Library Imports\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "from pydub import AudioSegment\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from tempfile import NamedTemporaryFile\n",
    "# from parler_tts import ParlerTTSForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Audio Conversion Function\n",
    "def convert_audio_to_wav(input_path, output_path=\"output.wav\", target_sample_rate=16000):\n",
    "    try:\n",
    "        command = [\n",
    "            \"ffmpeg\", \"-i\", input_path,\n",
    "            \"-ar\", str(target_sample_rate),\n",
    "            \"-ac\", \"1\",\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Converted {input_path} to {output_path} at {target_sample_rate} Hz.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred during conversion:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def convert_audio_to_wav(input_path, output_path=\"output.wav\"):\n",
    "    try:\n",
    "        temp_file = NamedTemporaryFile(delete=False, suffix=\".tmp\")\n",
    "        file_location = temp_file.name\n",
    "        print(temp_file.name)\n",
    "        with open(file_location, \"wb\") as f:\n",
    "            f.write(await input_path)\n",
    "        \n",
    "        # Convert the file to WAV format\n",
    "        wav_file_location = file_location + \".wav\"\n",
    "        try:\n",
    "            audio = AudioSegment.from_file(file_location)\n",
    "            audio.export(output_path, format=\"wav\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting file to WAV: {e}\")\n",
    "            return JSONResponse(content={\"error\": \"Failed to convert file to WAV\"}, status_code=500)\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content={\"error\": \"Failed to convert file to WAV\"}, status_code=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_wav_file = \"outputFile.wav\"\n",
    "# Transcribe the audio\n",
    "model_repo = \"shReYas0363/whisper-tiny-fine-tuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_repo)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Transcription Function\n",
    "def transcribe_audio(audio_path, model_repo):\n",
    "\n",
    "\n",
    "    #PREDEFINED UNTIL NOW\n",
    "    \n",
    "    audio_input, sampling_rate = sf.read(audio_path)\n",
    "    \n",
    "    if len(audio_input.shape) > 1:\n",
    "        audio_input = audio_input.mean(axis=1)\n",
    "    \n",
    "    input_features = processor(audio_input, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    input_features = input_features.to(device)\n",
    "    \n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: GROQ Chat Completion\n",
    "def generate_chat_response(transcription):\n",
    "    client = Groq(api_key='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')\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f'''You are a medical assistant give a very friendly one line response for this query: {transcription} Remember to give a single reply in just one reply'''}],\n",
    "        model=\"llama-3.2-1b-preview\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 5: Text-to-Speech (TTS) Functionality\n",
    "# def synthesize_speech(prompt):\n",
    "#     device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
    "    \n",
    "#     description = \"Laura Female Indian voice normal\"\n",
    "#     input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "#     prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "#     generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "#     audio_arr = generation.cpu().numpy().squeeze()\n",
    "    \n",
    "#     sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object convert_audio_to_wav at 0x000002643ECBBAE0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Execution Cell\n",
    "# start_time = time.time()\n",
    "\n",
    "# Specify your audio file path\n",
    "input_audio_file = \"sample.m4a\"  \n",
    "output_wav_file = \"outputFile.wav\"\n",
    "\n",
    "# Convert audio to WAV\n",
    "convert_audio_to_wav(input_audio_file, output_wav_file)\n",
    "\n",
    "\n",
    "# yet to actually implement this for the .m4a to .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Hello, my check123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transcription = transcribe_audio(output_wav_file, model_repo)\n",
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Response: You have a recent check-in and we're all set to review your health records as of today!\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate chat response\n",
    "resps = generate_chat_response(transcription)\n",
    "print(\"Chat Response:\", resps)\n",
    "\n",
    "# Synthesize speech\n",
    "# synthesize_speech(resps)\n",
    "\n",
    "# Measure execution time\n",
    "# end_time = time.time()\n",
    "# print(f\"The total time taken is: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# # Cleanup\n",
    "# os.remove(output_wav_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV file has been saved as 'output_wav.wav'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL and parameters\n",
    "url = \"http://[::1]:5002/api/tts\"\n",
    "params = {\n",
    "    \"text\": resps,\n",
    "    \"speaker_id\": \"p374\",\n",
    "    \"style_wav\": \"\",\n",
    "    \"language_id\": \"\"\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Write the binary response content to a WAV file\n",
    "    with open('output_wav.wav', 'wb') as wav_file:\n",
    "        wav_file.write(response.content)\n",
    "    print(\"WAV file has been saved as 'output_wav.wav'.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU (Python 3.10)",
   "language": "python",
   "name": "pytorch-gpu-python-3-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
