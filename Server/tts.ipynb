{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Library Imports\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Audio Conversion Function\n",
    "def convert_audio_to_wav(input_path, output_path=\"output.wav\", target_sample_rate=16000):\n",
    "    try:\n",
    "        command = [\n",
    "            \"ffmpeg\", \"-i\", input_path,\n",
    "            \"-ar\", str(target_sample_rate),\n",
    "            \"-ac\", \"1\",\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Converted {input_path} to {output_path} at {target_sample_rate} Hz.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred during conversion:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Transcription Function\n",
    "def transcribe_audio(audio_path, model_repo):\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_repo)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    audio_input, sampling_rate = sf.read(audio_path)\n",
    "    \n",
    "    if len(audio_input.shape) > 1:\n",
    "        audio_input = audio_input.mean(axis=1)\n",
    "    \n",
    "    input_features = processor(audio_input, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    input_features = input_features.to(device)\n",
    "    \n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: GROQ Chat Completion\n",
    "def generate_chat_response(transcription):\n",
    "    client = Groq(api_key='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')\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f'''You are a medical assistant give a very friendly one line response for this query: {transcription} Remember to give a single reply in just one reply'''}],\n",
    "        model=\"llama-3.2-1b-preview\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Text-to-Speech (TTS) Functionality\n",
    "def synthesize_speech(prompt):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
    "    \n",
    "    description = \"Laura Female Indian voice normal\"\n",
    "    input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "    \n",
    "    sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Execution Cell\n",
    "start_time = time.time()\n",
    "\n",
    "# Specify your audio file path\n",
    "input_audio_file = \"sample.m4a\"  \n",
    "output_wav_file = \"outputFile.wav\"\n",
    "\n",
    "# Convert audio to WAV\n",
    "convert_audio_to_wav(input_audio_file, output_wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Hello, my check123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_wav_file = \"outputFile.wav\"\n",
    "# Transcribe the audio\n",
    "model_repo = \"shReYas0363/whisper-tiny-fine-tuned\"\n",
    "transcription = transcribe_audio(output_wav_file, model_repo)\n",
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Response: \"Bhookh hai, check karna thoda accha hai,  kanoon mein ya, apne doctor se check karna puchh le!\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate chat response\n",
    "response = generate_chat_response(transcription)\n",
    "print(\"Chat Response:\", response)\n",
    "\n",
    "# Synthesize speech\n",
    "synthesize_speech(response)\n",
    "\n",
    "# Measure execution time\n",
    "# end_time = time.time()\n",
    "# print(f\"The total time taken is: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# # Cleanup\n",
    "# os.remove(output_wav_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
